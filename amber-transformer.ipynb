{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import uuid\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb73bb44150>"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_size = 512\n",
    "hidden_layer_size = 512\n",
    "batch_size = 10\n",
    "num_epochs = 70\n",
    "vocab_size = 102\n",
    "# max_sequence_length = 4123\n",
    "learning_rate= 0.01\n",
    "data_sample_size = 100\n",
    "uuid = random_uuid = uuid.uuid4()\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and train sentencepiece on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sentences.txt\n",
      "  input_format: \n",
      "  model_prefix: sentences\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 102\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: sentences.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 446 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=75543\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9563% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=55\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999563\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 444 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=42343\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 3962 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 444\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 2470\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 2470 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1731 obj=10.1233 num_tokens=4922 num_tokens/piece=2.84344\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1469 obj=8.52811 num_tokens=4948 num_tokens/piece=3.36828\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1101 obj=8.62872 num_tokens=5395 num_tokens/piece=4.90009\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1099 obj=8.54679 num_tokens=5409 num_tokens/piece=4.92175\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=824 obj=9.04159 num_tokens=6272 num_tokens/piece=7.61165\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=824 obj=8.93498 num_tokens=6270 num_tokens/piece=7.60922\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=618 obj=9.78364 num_tokens=7546 num_tokens/piece=12.2104\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=618 obj=9.65078 num_tokens=7548 num_tokens/piece=12.2136\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=463 obj=10.4759 num_tokens=8865 num_tokens/piece=19.1469\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=463 obj=10.353 num_tokens=8865 num_tokens/piece=19.1469\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=347 obj=11.1258 num_tokens=10060 num_tokens/piece=28.9914\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=347 obj=10.9331 num_tokens=10060 num_tokens/piece=28.9914\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=260 obj=11.644 num_tokens=11126 num_tokens/piece=42.7923\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=260 obj=11.5314 num_tokens=11126 num_tokens/piece=42.7923\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=195 obj=12.3242 num_tokens=12006 num_tokens/piece=61.5692\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=195 obj=12.2028 num_tokens=12006 num_tokens/piece=61.5692\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=146 obj=13.1913 num_tokens=13028 num_tokens/piece=89.2329\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=146 obj=13.0288 num_tokens=13028 num_tokens/piece=89.2329\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=112 obj=14.0904 num_tokens=13975 num_tokens/piece=124.777\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=112 obj=13.8767 num_tokens=13975 num_tokens/piece=124.777\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: sentences.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: sentences.vocab\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "sample_data = dataset[\"train\"]['text'][:data_sample_size]\n",
    "\n",
    "max_sequence_length = max(len(sentence) for sentence in train_data)\n",
    "print(max_sequence_length)\n",
    "\n",
    "with open('sentences.txt', 'w') as f:\n",
    "    for sentence in sample_data:\n",
    "        f.write(sentence + '\\n')  # Write the sentence to the file with a newline\n",
    "\n",
    "# with open(\"sentences.txt\", \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# lines = [line for line in lines if line.strip()]\n",
    "\n",
    "# with open(\"sentences.txt\", \"w\") as f:\n",
    "#     f.writelines(lines)\n",
    "\n",
    "input_file = 'sentences.txt' \n",
    "prefix = 'sentences'\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=input_file, \n",
    "    model_prefix=prefix, \n",
    "    vocab_size=vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and initialise the sentencepiece tokenizer\n",
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = spm.SentencePieceProcessor(model_file=model_path)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text, out_type=int)\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode_pieces(tokens)\n",
    "\n",
    "model_path = \"sentences.model\"\n",
    "sentence_piece_tokenizer = SentencePieceTokenizer(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    f = open('sentences.txt', 'r')\n",
    "    self.stories = f.read().split('\\n')\n",
    "    f.close()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.stories)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    story = self.stories[idx]\n",
    "    return torch.tensor(sentence_piece_tokenizer.encode(story))\n",
    "  \n",
    "def collate_fn(batch):\n",
    "    # Sort batch by sequence length (descending order)\n",
    "    batch = sorted(batch, key=lambda x: len(x), reverse=True)\n",
    "    # Pad sequences to the same length\n",
    "    padded_sequences = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "ds = Dataset()\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the simple transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SingleHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        # Mask tensor trick\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(max_sequence_length, max_sequence_length)))\n",
    "\n",
    "        self.query = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.key = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.value = torch.nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Linear transformations to obtain query, key, and value\n",
    "        queries = self.query(embeddings)\n",
    "        keys = self.key(embeddings)\n",
    "        values = self.value(embeddings)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        attention_scores_scaled = attention_scores / np.sqrt(self.embed_size)  # Apply scaling\n",
    "\n",
    "        msk = self.mask[0:x.shape[0], 0:x.shape[0]]\n",
    "        attention_masked = attention_scores_scaled.masked_fill(msk == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_masked, dim=-1)\n",
    "\n",
    "        # Weighted sum of values to get attended representation\n",
    "        attended_representation = torch.matmul(attention_weights, values)\n",
    "\n",
    "        return attended_representation\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmberSimpleTransformer(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(AmberSimpleTransformer, self).__init__()\n",
    "    # Embedding part of the model\n",
    "    self.embedding    = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "    self.pos_emb      = self.get_pos_matrix()\n",
    "\n",
    "    # attentions & norm\n",
    "    self.self_attention_00 = SingleHeadSelfAttention(embedding_size)\n",
    "    self.layer_norm_00 = torch.nn.LayerNorm(embedding_size) #is it ok for both to use the same layernorm? \n",
    "\n",
    "    self.self_attention_01 = SingleHeadSelfAttention(embedding_size)\n",
    "    self.layer_norm_01 = torch.nn.LayerNorm(embedding_size) #is it ok for both to use the same layernorm? \n",
    "    \n",
    "    #feedforward&norm\n",
    "    self.feedforward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_size, 2048),  # d_ff = 2048\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, embedding_size),\n",
    "        )   \n",
    "    self.norm = torch.nn.LayerNorm(embedding_size)\n",
    "\n",
    "    # last linear layer\n",
    "    self.map_to_vocab = torch.nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    emb = self.embedding(x) #embedding_size --> this will output something of d sequence length times embedding size \n",
    "    pos = self.pos_emb[0:x.shape[0], :]\n",
    "    emb = emb + pos # add positional information to the embedding \n",
    "\n",
    "    # masked sa + add&norm\n",
    "    res_0 = self.self_attention_00(emb)\n",
    "    res_0 = self.layer_norm_00(emb + res_0)\n",
    "\n",
    "    # masked sa + add&norm\n",
    "    res_1 = self.self_attention_01(res_0)\n",
    "    res_1 = self.layer_norm_01(res_0 + res_1)\n",
    "\n",
    "    #feed forward + add&norm\n",
    "    final_res = self.feedforward(res_1)\n",
    "    final_res = self.norm(final_res)\n",
    "\n",
    "    #linear then softmax\n",
    "    out = self.map_to_vocab(final_res)\n",
    "    # out = F.softmax(out, dim=-1) ---> Cross-entropy-loss does softmax, so we don't want to softmax before output\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "  def get_pos_matrix(self):\n",
    "    store = torch.zeros(max_sequence_length, embedding_size)\n",
    "    for pos in range(max_sequence_length):\n",
    "      for i in range(0, embedding_size, 2):\n",
    "        denominator = 10000 ** (2 * i / embedding_size)\n",
    "        store[pos, i] = math.sin(pos / denominator)\n",
    "        if i + 1 < embedding_size: store[pos, i + 1] = math.cos(pos / denominator)\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise and train the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"amber-transformer\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config= {\n",
    "#     \"architecture\": \"transformer\",\n",
    "#     \"dataset\": \"roneneldan/TinyStories\",\n",
    "#     \"embedding_size\":embedding_size,\n",
    "#     \"hidden_layer_size\": hidden_layer_size, \n",
    "#     \"batch_size\": batch_size,\n",
    "#     \"num_epochs\": num_epochs,\n",
    "#     \"vocab_size\":vocab_size,\n",
    "#     \"learning_rate\":learning_rate,\n",
    "#     \"data_sample_size\": data_sample_size,\n",
    "#     \"optimiser\": \"adam\",\n",
    "#     \"uuid\": uuid\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_params: 3783270\n",
      "Loss: 4.649833679199219\n",
      "Loss: 7.611556053161621\n",
      "Loss: 4.58792781829834\n",
      "Loss: 5.558635711669922\n",
      "Loss: 5.754090309143066\n",
      "Loss: 5.897739887237549\n",
      "Loss: 6.408228397369385\n",
      "Loss: 5.762607097625732\n",
      "Loss: 6.0188212394714355\n",
      "Loss: 5.5475006103515625\n",
      "Loss: 5.067815780639648\n",
      "Loss: 4.600011825561523\n",
      "Loss: 4.384387016296387\n",
      "Loss: 5.199957370758057\n",
      "Loss: 5.103014945983887\n",
      "Loss: 5.3800249099731445\n",
      "Loss: 4.817124843597412\n",
      "Loss: 4.479198932647705\n",
      "Loss: 4.653329849243164\n",
      "Loss: 4.248904705047607\n",
      "Loss: 4.574100017547607\n",
      "Loss: 4.623258590698242\n",
      "Loss: 4.519676685333252\n",
      "Loss: 4.379165172576904\n",
      "Loss: 4.40183162689209\n",
      "Loss: 4.227741241455078\n",
      "Loss: 4.498483180999756\n",
      "Loss: 4.308802604675293\n",
      "Loss: 4.219172954559326\n",
      "Loss: 4.158656120300293\n",
      "Loss: 4.108094692230225\n",
      "Loss: 4.313732624053955\n",
      "Loss: 4.328035354614258\n",
      "Loss: 4.272812366485596\n",
      "Loss: 4.025577068328857\n",
      "Loss: 4.361980438232422\n",
      "Loss: 4.062005043029785\n",
      "Loss: 4.339924335479736\n",
      "Loss: 4.069060325622559\n",
      "Loss: 4.260838985443115\n",
      "Loss: 3.8062021732330322\n",
      "Loss: 3.950432300567627\n",
      "Loss: 3.7978975772857666\n",
      "Loss: 4.068914890289307\n",
      "Loss: 4.356741905212402\n",
      "Loss: 4.024046897888184\n",
      "Loss: 3.953092575073242\n",
      "Loss: 4.020203590393066\n",
      "Loss: 3.8295438289642334\n",
      "Loss: 4.064754486083984\n",
      "Loss: 3.8718934059143066\n",
      "Loss: 4.055598735809326\n",
      "Loss: 4.176519870758057\n",
      "Loss: 3.962308406829834\n",
      "Loss: 4.46663236618042\n",
      "Loss: 4.055781364440918\n",
      "Loss: 3.882777690887451\n",
      "Loss: 3.848240375518799\n",
      "Loss: 3.983710527420044\n",
      "Loss: 3.8347320556640625\n",
      "Loss: 3.8212926387786865\n",
      "Loss: 4.0853776931762695\n",
      "Loss: 4.172112941741943\n",
      "Loss: 3.7456164360046387\n",
      "Loss: 3.9457342624664307\n",
      "Loss: 3.7313506603240967\n",
      "Loss: 3.89799165725708\n",
      "Loss: 3.887515068054199\n",
      "Loss: 3.883396863937378\n",
      "Loss: 3.9076478481292725\n",
      "Loss: 3.9160804748535156\n",
      "Loss: 3.6666743755340576\n",
      "Loss: 3.9849250316619873\n",
      "Loss: 4.236770153045654\n",
      "Loss: 3.6609835624694824\n",
      "Loss: 3.9471514225006104\n",
      "Loss: 3.761770486831665\n",
      "Loss: 3.820833921432495\n",
      "Loss: 3.9376471042633057\n",
      "Loss: 3.8852760791778564\n",
      "Loss: 4.13874626159668\n",
      "Loss: 3.7836155891418457\n",
      "Loss: 3.964075803756714\n",
      "Loss: 3.7133185863494873\n",
      "Loss: 3.783454179763794\n",
      "Loss: 3.7843780517578125\n",
      "Loss: 3.929431676864624\n",
      "Loss: 3.721673011779785\n",
      "Loss: 3.8400001525878906\n",
      "Loss: 4.0188517570495605\n",
      "Loss: 4.301368236541748\n",
      "Loss: 4.024944305419922\n",
      "Loss: 4.040686130523682\n",
      "Loss: 3.9606523513793945\n",
      "Loss: 3.960557460784912\n",
      "Loss: 3.9428133964538574\n",
      "Loss: 3.786848545074463\n",
      "Loss: 3.8591105937957764\n",
      "Loss: 4.0041375160217285\n",
      "Loss: 4.347339153289795\n",
      "Loss: 3.843778610229492\n",
      "Loss: 3.9268856048583984\n",
      "Loss: 3.9719250202178955\n",
      "Loss: 3.9246485233306885\n",
      "Loss: 3.7254106998443604\n",
      "Loss: 3.8447961807250977\n",
      "Loss: 4.182858943939209\n",
      "Loss: 3.910426378250122\n",
      "Loss: 3.704174757003784\n",
      "Loss: 3.900089740753174\n",
      "Loss: 4.082404136657715\n",
      "Loss: 3.9811270236968994\n",
      "Loss: 3.7276041507720947\n",
      "Loss: 3.7839667797088623\n",
      "Loss: 3.9247193336486816\n",
      "Loss: 3.786311626434326\n",
      "Loss: 3.9067108631134033\n",
      "Loss: 4.147678375244141\n",
      "Loss: 4.103945732116699\n",
      "Loss: 3.976989269256592\n",
      "Loss: 3.6335573196411133\n",
      "Loss: 3.7515575885772705\n",
      "Loss: 3.840709924697876\n",
      "Loss: 3.783933401107788\n",
      "Loss: 3.6231675148010254\n",
      "Loss: 3.7666983604431152\n",
      "Loss: 3.8123669624328613\n",
      "Loss: 4.134504318237305\n",
      "Loss: 3.9442059993743896\n",
      "Loss: 3.7970659732818604\n",
      "Loss: 3.8665871620178223\n",
      "Loss: 3.6739420890808105\n",
      "Loss: 3.8526735305786133\n",
      "Loss: 3.879096031188965\n",
      "Loss: 3.776160717010498\n",
      "Loss: 3.8642759323120117\n",
      "Loss: 3.869023323059082\n",
      "Loss: 3.7266998291015625\n",
      "Loss: 4.025785446166992\n",
      "Loss: 3.721226930618286\n",
      "Loss: 3.690948724746704\n",
      "Loss: 4.10778284072876\n",
      "Loss: 3.6989171504974365\n",
      "Loss: 3.750030517578125\n",
      "Loss: 3.7084474563598633\n",
      "Loss: 3.8369576930999756\n",
      "Loss: 4.09893274307251\n",
      "Loss: 3.777325391769409\n",
      "Loss: 3.626314878463745\n",
      "Loss: 3.791630744934082\n",
      "Loss: 3.706048011779785\n",
      "Loss: 3.841268301010132\n",
      "Loss: 3.7073919773101807\n",
      "Loss: 3.8318276405334473\n",
      "Loss: 3.810673952102661\n",
      "Loss: 3.830078125\n",
      "Loss: 4.039241313934326\n",
      "Loss: 3.9575390815734863\n",
      "Loss: 3.7888684272766113\n",
      "Loss: 3.7310807704925537\n",
      "Loss: 3.8052892684936523\n",
      "Loss: 3.8980469703674316\n",
      "Loss: 3.7421469688415527\n",
      "Loss: 3.839412212371826\n",
      "Loss: 3.82466197013855\n",
      "Loss: 3.8366475105285645\n",
      "Loss: 3.801309585571289\n",
      "Loss: 3.70521879196167\n",
      "Loss: 3.664499044418335\n",
      "Loss: 3.8603477478027344\n",
      "Loss: 3.7188329696655273\n",
      "Loss: 4.071581840515137\n",
      "Loss: 3.9030094146728516\n",
      "Loss: 4.034590721130371\n",
      "Loss: 3.8702232837677\n",
      "Loss: 3.8377535343170166\n",
      "Loss: 3.6870508193969727\n",
      "Loss: 3.9831395149230957\n",
      "Loss: 3.6282291412353516\n",
      "Loss: 3.6155903339385986\n",
      "Loss: 3.841282606124878\n",
      "Loss: 3.7842702865600586\n",
      "Loss: 3.703660249710083\n",
      "Loss: 3.771450996398926\n",
      "Loss: 3.8802523612976074\n",
      "Loss: 3.7825045585632324\n",
      "Loss: 3.795346260070801\n",
      "Loss: 3.714327812194824\n",
      "Loss: 3.649183511734009\n",
      "Loss: 3.635443925857544\n",
      "Loss: 3.8188164234161377\n",
      "Loss: 4.332197666168213\n",
      "Loss: 3.8391060829162598\n",
      "Loss: 3.821215867996216\n",
      "Loss: 3.6649866104125977\n",
      "Loss: 3.7381532192230225\n",
      "Loss: 3.7085201740264893\n",
      "Loss: 3.6774306297302246\n",
      "Loss: 3.814526081085205\n",
      "Loss: 3.7598049640655518\n",
      "Loss: 3.7682278156280518\n",
      "Loss: 3.9064693450927734\n",
      "Loss: 3.8248798847198486\n",
      "Loss: 3.7721357345581055\n",
      "Loss: 3.906900644302368\n",
      "Loss: 4.182851791381836\n",
      "Loss: 3.870150327682495\n",
      "Loss: 3.8655669689178467\n",
      "Loss: 3.970811367034912\n",
      "Loss: 3.7364702224731445\n",
      "Loss: 3.7495856285095215\n",
      "Loss: 3.859583616256714\n",
      "Loss: 3.801452875137329\n",
      "Loss: 3.8064382076263428\n",
      "Loss: 3.9797377586364746\n",
      "Loss: 3.677363634109497\n",
      "Loss: 3.664231300354004\n",
      "Loss: 3.8647572994232178\n",
      "Loss: 3.8871641159057617\n",
      "Loss: 3.7864606380462646\n",
      "Loss: 3.6835782527923584\n",
      "Loss: 3.920652389526367\n",
      "Loss: 3.833056926727295\n",
      "Loss: 3.7131998538970947\n",
      "Loss: 3.7270867824554443\n",
      "Loss: 3.716475486755371\n",
      "Loss: 3.768892288208008\n",
      "Loss: 3.8469038009643555\n",
      "Loss: 3.7219886779785156\n",
      "Loss: 3.7563440799713135\n",
      "Loss: 4.111198425292969\n",
      "Loss: 3.79441237449646\n",
      "Loss: 4.017744541168213\n",
      "Loss: 3.6813747882843018\n",
      "Loss: 3.88777756690979\n",
      "Loss: 3.7991678714752197\n",
      "Loss: 3.7833807468414307\n",
      "Loss: 4.03745698928833\n",
      "Loss: 3.8116214275360107\n",
      "Loss: 3.645655632019043\n",
      "Loss: 3.71126127243042\n",
      "Loss: 3.835155487060547\n",
      "Loss: 3.845424175262451\n",
      "Loss: 3.8903427124023438\n",
      "Loss: 4.030483245849609\n",
      "Loss: 3.8617429733276367\n",
      "Loss: 3.8760159015655518\n",
      "Loss: 3.8017208576202393\n",
      "Loss: 3.6878325939178467\n",
      "Loss: 3.9256045818328857\n",
      "Loss: 3.77228045463562\n",
      "Loss: 3.9212284088134766\n",
      "Loss: 3.9607174396514893\n",
      "Loss: 3.8304994106292725\n",
      "Loss: 3.7694458961486816\n",
      "Loss: 3.842449426651001\n",
      "Loss: 4.280496120452881\n",
      "Loss: 3.825997829437256\n",
      "Loss: 3.922133207321167\n",
      "Loss: 3.7850067615509033\n",
      "Loss: 3.8271677494049072\n",
      "Loss: 3.6993179321289062\n",
      "Loss: 3.790940523147583\n",
      "Loss: 4.003471851348877\n",
      "Loss: 3.8946475982666016\n",
      "Loss: 3.8102829456329346\n",
      "Loss: 3.6928164958953857\n",
      "Loss: 3.8581149578094482\n",
      "Loss: 3.7910382747650146\n",
      "Loss: 3.815821886062622\n",
      "Loss: 3.7653284072875977\n",
      "Loss: 3.995204210281372\n",
      "Loss: 3.7569549083709717\n",
      "Loss: 3.8892147541046143\n",
      "Loss: 3.853585720062256\n",
      "Loss: 3.6582512855529785\n",
      "Loss: 3.7550926208496094\n",
      "Loss: 3.7368156909942627\n",
      "Loss: 3.6770687103271484\n",
      "Loss: 3.9269447326660156\n",
      "Loss: 3.9385812282562256\n",
      "Loss: 3.9258198738098145\n",
      "Loss: 3.7217202186584473\n",
      "Loss: 4.0809221267700195\n",
      "Loss: 3.8424832820892334\n",
      "Loss: 3.725978374481201\n",
      "Loss: 3.6652588844299316\n",
      "Loss: 3.8661844730377197\n",
      "Loss: 3.6803975105285645\n",
      "Loss: 3.9625213146209717\n",
      "Loss: 3.841120958328247\n",
      "Loss: 3.72920560836792\n",
      "Loss: 3.9205925464630127\n",
      "Loss: 3.720350980758667\n",
      "Loss: 3.873656988143921\n",
      "Loss: 3.752131938934326\n",
      "Loss: 3.8970916271209717\n",
      "Loss: 3.6652770042419434\n",
      "Loss: 3.7065792083740234\n",
      "Loss: 3.8175759315490723\n",
      "Loss: 3.8592185974121094\n",
      "Loss: 3.8851699829101562\n",
      "Loss: 3.892805814743042\n",
      "Loss: 3.61545729637146\n",
      "Loss: 3.8317904472351074\n",
      "Loss: 3.9087634086608887\n",
      "Loss: 3.675687789916992\n",
      "Loss: 3.7101244926452637\n",
      "Loss: 3.652378797531128\n",
      "Loss: 3.7238211631774902\n",
      "Loss: 3.674346685409546\n",
      "Loss: 3.8373639583587646\n",
      "Loss: 4.2743000984191895\n",
      "Loss: 3.6887197494506836\n",
      "Loss: 3.8886537551879883\n",
      "Loss: 3.6801364421844482\n",
      "Loss: 3.790614128112793\n",
      "Loss: 3.7081286907196045\n",
      "Loss: 3.840757131576538\n",
      "Loss: 3.8243160247802734\n",
      "Loss: 3.7071304321289062\n",
      "Loss: 3.8082776069641113\n",
      "Loss: 3.668914556503296\n",
      "Loss: 3.8614187240600586\n",
      "Loss: 3.7687270641326904\n",
      "Loss: 3.6585299968719482\n",
      "Loss: 3.76310396194458\n",
      "Loss: 3.8162524700164795\n",
      "Loss: 3.796264171600342\n",
      "Loss: 3.7115325927734375\n",
      "Loss: 3.6896307468414307\n",
      "Loss: 3.738631248474121\n",
      "Loss: 3.920870542526245\n",
      "Loss: 3.8600594997406006\n",
      "Loss: 3.7773783206939697\n",
      "Loss: 3.680413246154785\n",
      "Loss: 3.7316362857818604\n",
      "Loss: 3.7399442195892334\n",
      "Loss: 3.70206880569458\n",
      "Loss: 3.99527907371521\n",
      "Loss: 3.8968613147735596\n",
      "Loss: 3.692972421646118\n",
      "Loss: 3.7842071056365967\n",
      "Loss: 3.7273383140563965\n",
      "Loss: 3.876378297805786\n",
      "Loss: 3.63489031791687\n",
      "Loss: 3.669962167739868\n",
      "Loss: 3.8900763988494873\n",
      "Loss: 3.702667474746704\n",
      "Loss: 3.820606231689453\n",
      "Loss: 3.813753366470337\n",
      "Loss: 3.6649835109710693\n",
      "Loss: 3.72489595413208\n",
      "Loss: 3.651012897491455\n",
      "Loss: 3.6568527221679688\n",
      "Loss: 3.733811378479004\n",
      "Loss: 3.5806071758270264\n",
      "Loss: 3.657243251800537\n",
      "Loss: 3.709725856781006\n",
      "Loss: 3.6053433418273926\n",
      "Loss: 3.8025572299957275\n",
      "Loss: 3.9515271186828613\n",
      "Loss: 3.827514410018921\n",
      "Loss: 3.69110369682312\n",
      "Loss: 3.6184213161468506\n",
      "Loss: 3.663512945175171\n",
      "Loss: 3.6546287536621094\n",
      "Loss: 3.814974308013916\n",
      "Loss: 3.940131664276123\n",
      "Loss: 3.6798255443573\n",
      "Loss: 3.903627872467041\n",
      "Loss: 3.8360917568206787\n",
      "Loss: 3.68105149269104\n",
      "Loss: 3.767836332321167\n",
      "Loss: 3.701235294342041\n",
      "Loss: 3.736575126647949\n",
      "Loss: 3.731428623199463\n",
      "Loss: 3.7318668365478516\n",
      "Loss: 3.62528395652771\n",
      "Loss: 3.7358696460723877\n",
      "Loss: 3.744544744491577\n",
      "Loss: 3.669283628463745\n",
      "Loss: 3.696096658706665\n",
      "Loss: 3.7078638076782227\n",
      "Loss: 3.8364157676696777\n",
      "Loss: 3.6919820308685303\n",
      "Loss: 3.816288948059082\n",
      "Loss: 3.633277177810669\n",
      "Loss: 3.8405683040618896\n",
      "Loss: 3.77192759513855\n",
      "Loss: 3.7384109497070312\n",
      "Loss: 3.9517908096313477\n",
      "Loss: 3.786109685897827\n",
      "Loss: 3.700129270553589\n",
      "Loss: 3.7041220664978027\n",
      "Loss: 3.8971807956695557\n",
      "Loss: 3.698352336883545\n",
      "Loss: 3.757683515548706\n",
      "Loss: 3.968776226043701\n",
      "Loss: 3.6767776012420654\n",
      "Loss: 3.6902761459350586\n",
      "Loss: 3.805285692214966\n",
      "Loss: 4.226135730743408\n",
      "Loss: 3.793714761734009\n",
      "Loss: 3.8956775665283203\n",
      "Loss: 3.6993870735168457\n",
      "Loss: 3.76826548576355\n",
      "Loss: 3.8282809257507324\n",
      "Loss: 3.706491231918335\n",
      "Loss: 3.8163840770721436\n",
      "Loss: 3.6280720233917236\n",
      "Loss: 3.821903944015503\n",
      "Loss: 3.844019651412964\n",
      "Loss: 3.7068538665771484\n",
      "Loss: 3.6619420051574707\n",
      "Loss: 3.9919495582580566\n",
      "Loss: 3.941643238067627\n",
      "Loss: 3.825787305831909\n",
      "Loss: 3.714873790740967\n",
      "Loss: 3.680466413497925\n",
      "Loss: 3.7289695739746094\n",
      "Loss: 3.788978099822998\n",
      "Loss: 3.8271350860595703\n",
      "Loss: 3.661818027496338\n",
      "Loss: 3.6963870525360107\n",
      "Loss: 3.6398282051086426\n",
      "Loss: 3.6944684982299805\n",
      "Loss: 3.7372798919677734\n",
      "Loss: 3.7753055095672607\n",
      "Loss: 3.719999074935913\n",
      "Loss: 3.8622162342071533\n",
      "Loss: 3.623410940170288\n",
      "Loss: 3.6481454372406006\n",
      "Loss: 4.098227024078369\n",
      "Loss: 3.595008373260498\n",
      "Loss: 3.7733795642852783\n",
      "Loss: 3.7736053466796875\n",
      "Loss: 3.793911933898926\n",
      "Loss: 3.8094544410705566\n",
      "Loss: 3.8587920665740967\n",
      "Loss: 3.8033249378204346\n",
      "Loss: 3.8745205402374268\n",
      "Loss: 3.6704094409942627\n",
      "Loss: 3.938354015350342\n",
      "Loss: 3.929546356201172\n",
      "Loss: 3.798448085784912\n",
      "Loss: 3.7542314529418945\n",
      "Loss: 3.9670310020446777\n",
      "Loss: 3.7802624702453613\n",
      "Loss: 3.6974573135375977\n",
      "Loss: 3.753474235534668\n",
      "Loss: 3.7736072540283203\n",
      "Loss: 3.8442819118499756\n",
      "Loss: 3.6984360218048096\n",
      "Loss: 3.6823008060455322\n",
      "Loss: 3.7666127681732178\n",
      "Loss: 3.9332573413848877\n",
      "Loss: 3.694537401199341\n",
      "Loss: 3.7403981685638428\n",
      "Loss: 4.06651496887207\n",
      "Loss: 3.748060464859009\n",
      "Loss: 3.6929306983947754\n",
      "Loss: 3.782778739929199\n",
      "Loss: 3.926570177078247\n",
      "Loss: 3.7975058555603027\n",
      "Loss: 3.8943021297454834\n",
      "Loss: 3.677875280380249\n",
      "Loss: 3.6042912006378174\n",
      "Loss: 3.6705143451690674\n",
      "Loss: 3.6502392292022705\n",
      "Loss: 3.8084163665771484\n",
      "Loss: 3.8116486072540283\n",
      "Loss: 3.6488170623779297\n",
      "Loss: 3.7477917671203613\n",
      "Loss: 3.840047836303711\n",
      "Loss: 3.8647711277008057\n",
      "Loss: 3.6999614238739014\n",
      "Loss: 4.083037376403809\n",
      "Loss: 3.9484429359436035\n",
      "Loss: 3.6516501903533936\n",
      "Loss: 3.7905664443969727\n",
      "Loss: 4.0147199630737305\n",
      "Loss: 3.7319273948669434\n",
      "Loss: 3.6417858600616455\n",
      "Loss: 3.8169894218444824\n",
      "Loss: 3.7302968502044678\n",
      "Loss: 3.7846007347106934\n",
      "Loss: 3.7057275772094727\n",
      "Loss: 3.724020481109619\n",
      "Loss: 3.939849853515625\n",
      "Loss: 3.8063595294952393\n",
      "Loss: 3.6778247356414795\n",
      "Loss: 3.739204168319702\n",
      "Loss: 3.7644307613372803\n",
      "Loss: 3.757760763168335\n",
      "Loss: 3.6593332290649414\n",
      "Loss: 3.6988515853881836\n",
      "Loss: 3.7370595932006836\n",
      "Loss: 3.6413068771362305\n",
      "Loss: 3.8688929080963135\n",
      "Loss: 3.8234286308288574\n",
      "Loss: 3.7018747329711914\n",
      "Loss: 3.6537792682647705\n",
      "Loss: 3.6470494270324707\n",
      "Loss: 3.82283878326416\n",
      "Loss: 3.563272476196289\n",
      "Loss: 3.619906187057495\n",
      "Loss: 3.6006035804748535\n",
      "Loss: 3.730710029602051\n",
      "Loss: 3.609675645828247\n",
      "Loss: 3.574570894241333\n",
      "Loss: 3.855416774749756\n",
      "Loss: 3.756107807159424\n",
      "Loss: 3.6726837158203125\n",
      "Loss: 3.6390247344970703\n",
      "Loss: 3.6679985523223877\n",
      "Loss: 3.7479374408721924\n",
      "Loss: 3.765554428100586\n",
      "Loss: 3.633448839187622\n",
      "Loss: 3.708101987838745\n",
      "Loss: 3.851322889328003\n",
      "Loss: 3.904252767562866\n",
      "Loss: 3.825681447982788\n",
      "Loss: 3.970991611480713\n",
      "Loss: 3.9947023391723633\n",
      "Loss: 3.8099043369293213\n",
      "Loss: 3.791346788406372\n",
      "Loss: 3.7616491317749023\n",
      "Loss: 3.9221997261047363\n",
      "Loss: 3.663578987121582\n",
      "Loss: 3.9393367767333984\n",
      "Loss: 3.687666177749634\n",
      "Loss: 3.7288243770599365\n",
      "Loss: 3.8285250663757324\n",
      "Loss: 3.8699071407318115\n",
      "Loss: 3.7423877716064453\n",
      "Loss: 3.903541326522827\n",
      "Loss: 3.67547869682312\n",
      "Loss: 3.702547550201416\n",
      "Loss: 3.8395237922668457\n",
      "Loss: 3.658677339553833\n",
      "Loss: 3.8207931518554688\n",
      "Loss: 3.872401714324951\n",
      "Loss: 3.827928066253662\n",
      "Loss: 3.7005937099456787\n",
      "Loss: 3.7390758991241455\n",
      "Loss: 3.9421164989471436\n",
      "Loss: 3.7176010608673096\n",
      "Loss: 3.8950417041778564\n",
      "Loss: 3.6344640254974365\n",
      "Loss: 3.6475865840911865\n",
      "Loss: 3.790292501449585\n",
      "Loss: 3.6615869998931885\n",
      "Loss: 3.664991617202759\n",
      "Loss: 3.682314872741699\n",
      "Loss: 3.650312900543213\n",
      "Loss: 3.8188583850860596\n",
      "Loss: 3.751133680343628\n",
      "Loss: 3.817537784576416\n",
      "Loss: 3.587761163711548\n",
      "Loss: 3.5956146717071533\n",
      "Loss: 3.7844347953796387\n",
      "Loss: 3.6371967792510986\n",
      "Loss: 3.9415531158447266\n",
      "Loss: 3.938486099243164\n",
      "Loss: 3.814873695373535\n",
      "Loss: 3.7371954917907715\n",
      "Loss: 3.7805192470550537\n",
      "Loss: 3.7062411308288574\n",
      "Loss: 4.001285076141357\n",
      "Loss: 3.7595887184143066\n",
      "Loss: 4.198992729187012\n",
      "Loss: 3.798154592514038\n",
      "Loss: 3.924663782119751\n",
      "Loss: 3.7903871536254883\n",
      "Loss: 3.825261116027832\n",
      "Loss: 3.705960273742676\n",
      "Loss: 3.6909708976745605\n",
      "Loss: 3.938847064971924\n",
      "Loss: 3.850038766860962\n",
      "Loss: 3.7751755714416504\n",
      "Loss: 3.998169422149658\n",
      "Loss: 3.8153579235076904\n",
      "Loss: 3.8675644397735596\n",
      "Loss: 3.7080347537994385\n",
      "Loss: 3.681821584701538\n",
      "Loss: 3.782606601715088\n",
      "Loss: 3.7555453777313232\n",
      "Loss: 3.661839008331299\n",
      "Loss: 3.842747688293457\n",
      "Loss: 3.7169790267944336\n",
      "Loss: 3.7991127967834473\n",
      "Loss: 3.7211506366729736\n",
      "Loss: 3.900308847427368\n",
      "Loss: 3.865922451019287\n",
      "Loss: 3.663591146469116\n",
      "Loss: 3.762847900390625\n",
      "Loss: 3.7486233711242676\n",
      "Loss: 3.741168737411499\n",
      "Loss: 3.7388360500335693\n",
      "Loss: 3.7372138500213623\n",
      "Loss: 3.801424503326416\n",
      "Loss: 3.942263126373291\n",
      "Loss: 3.710305690765381\n",
      "Loss: 3.660317897796631\n",
      "Loss: 3.5873684883117676\n",
      "Loss: 3.995551347732544\n",
      "Loss: 3.801609516143799\n",
      "Loss: 3.681497097015381\n",
      "Loss: 3.6878726482391357\n",
      "Loss: 3.7017269134521484\n",
      "Loss: 3.865050792694092\n",
      "Loss: 3.6089320182800293\n",
      "Loss: 3.8663134574890137\n",
      "Loss: 3.770172357559204\n",
      "Loss: 3.8381099700927734\n",
      "Loss: 3.7713980674743652\n",
      "Loss: 3.7346975803375244\n",
      "Loss: 3.827688694000244\n",
      "Loss: 3.9048707485198975\n",
      "Loss: 3.848971366882324\n",
      "Loss: 3.8612003326416016\n",
      "Loss: 3.760770320892334\n",
      "Loss: 3.8942911624908447\n",
      "Loss: 3.6703500747680664\n",
      "Loss: 3.833503484725952\n",
      "Loss: 3.9408106803894043\n",
      "Loss: 3.679765462875366\n",
      "Loss: 3.7250492572784424\n",
      "Loss: 3.724665403366089\n",
      "Loss: 3.965862512588501\n",
      "Loss: 3.824218988418579\n",
      "Loss: 3.8677291870117188\n",
      "Loss: 3.806664228439331\n",
      "Loss: 3.672248125076294\n",
      "Loss: 3.8186538219451904\n",
      "Loss: 3.799299478530884\n",
      "Loss: 3.69899320602417\n",
      "Loss: 3.7705559730529785\n",
      "Loss: 3.908764362335205\n",
      "Loss: 3.73604154586792\n",
      "Loss: 3.703777313232422\n",
      "Loss: 3.6580650806427\n",
      "Loss: 3.881429672241211\n",
      "Loss: 3.743774175643921\n",
      "Loss: 3.544839859008789\n",
      "Loss: 3.6563992500305176\n",
      "Loss: 3.7610931396484375\n",
      "Loss: 3.7487215995788574\n",
      "Loss: 3.727768659591675\n",
      "Loss: 3.7282872200012207\n",
      "Loss: 3.752789258956909\n",
      "Loss: 3.8080251216888428\n",
      "Loss: 3.7742209434509277\n",
      "Loss: 3.6528143882751465\n",
      "Loss: 3.8715951442718506\n",
      "Loss: 3.6493420600891113\n",
      "Loss: 3.851823091506958\n",
      "Loss: 3.684699535369873\n",
      "Loss: 3.938673257827759\n",
      "Loss: 3.677966594696045\n",
      "Loss: 3.9775633811950684\n",
      "Loss: 3.80544114112854\n",
      "Loss: 3.7281248569488525\n",
      "Loss: 3.6889572143554688\n",
      "Loss: 3.7554192543029785\n",
      "Loss: 3.742819309234619\n",
      "Loss: 3.798936128616333\n",
      "Loss: 3.709613084793091\n",
      "Loss: 3.806166887283325\n",
      "Loss: 3.7795190811157227\n",
      "Loss: 3.7373387813568115\n",
      "Loss: 3.665755271911621\n",
      "Loss: 4.001605987548828\n",
      "Loss: 3.7973172664642334\n",
      "Loss: 3.8246772289276123\n",
      "Loss: 3.697740077972412\n",
      "Loss: 3.7859320640563965\n",
      "Loss: 3.5554394721984863\n",
      "Loss: 3.6803998947143555\n",
      "Loss: 3.6993165016174316\n",
      "Loss: 3.742366075515747\n",
      "Loss: 3.6950414180755615\n",
      "Loss: 4.44597053527832\n",
      "Loss: 3.77774715423584\n",
      "Loss: 3.650639772415161\n",
      "Loss: 3.6693530082702637\n",
      "Loss: 3.9082987308502197\n",
      "Loss: 3.725748062133789\n",
      "Loss: 4.049629211425781\n",
      "Loss: 3.760312795639038\n",
      "Loss: 3.657399892807007\n",
      "Loss: 3.846581220626831\n",
      "Loss: 3.627257823944092\n",
      "Loss: 3.6148014068603516\n",
      "Loss: 3.984107732772827\n",
      "Loss: 3.8515284061431885\n",
      "Loss: 3.827085018157959\n",
      "Loss: 3.6723008155822754\n",
      "Loss: 3.7466719150543213\n",
      "Loss: 3.603003978729248\n",
      "Loss: 3.8045272827148438\n",
      "Loss: 3.656243324279785\n",
      "Loss: 3.7039806842803955\n",
      "Loss: 3.755047559738159\n",
      "Loss: 3.76395583152771\n",
      "Loss: 3.83846116065979\n",
      "Loss: 3.712893009185791\n",
      "Loss: 3.7491798400878906\n",
      "Loss: 3.6750831604003906\n",
      "Loss: 3.6444318294525146\n",
      "Loss: 3.710108757019043\n",
      "Loss: 3.7107200622558594\n",
      "Loss: 3.8700027465820312\n",
      "Loss: 3.8252744674682617\n",
      "Loss: 3.7187118530273438\n",
      "Loss: 3.747577428817749\n",
      "Loss: 3.936777114868164\n",
      "Loss: 3.7518246173858643\n",
      "Loss: 3.831749677658081\n",
      "Loss: 3.9489352703094482\n",
      "Loss: 3.892345905303955\n",
      "Loss: 3.6729888916015625\n",
      "Loss: 3.989300489425659\n",
      "Loss: 3.8795864582061768\n",
      "Loss: 3.6967639923095703\n",
      "Loss: 3.8438189029693604\n",
      "Loss: 3.8484275341033936\n",
      "Loss: 3.727992057800293\n",
      "Loss: 3.818732738494873\n",
      "Loss: 3.783648729324341\n",
      "Loss: 3.600308895111084\n",
      "Loss: 3.6884870529174805\n",
      "Loss: 3.8789994716644287\n",
      "Loss: 3.7978339195251465\n",
      "Loss: 3.64862060546875\n",
      "Loss: 3.7877228260040283\n",
      "Loss: 3.657034158706665\n",
      "Loss: 3.7529876232147217\n",
      "Loss: 3.787093162536621\n",
      "Loss: 3.9353582859039307\n",
      "Loss: 3.6881768703460693\n",
      "Loss: 4.147076606750488\n",
      "Loss: 3.9351561069488525\n",
      "Loss: 3.800272226333618\n",
      "Loss: 3.980571985244751\n",
      "Loss: 3.819828748703003\n",
      "Loss: 3.75480055809021\n",
      "Loss: 3.72232985496521\n",
      "Loss: 3.6181623935699463\n",
      "Loss: 3.824641466140747\n",
      "Loss: 3.832092761993408\n",
      "Loss: 3.6533491611480713\n",
      "Loss: 3.8152358531951904\n",
      "Loss: 3.7725517749786377\n",
      "Loss: 3.728262424468994\n",
      "Loss: 3.6979546546936035\n",
      "Loss: 3.7517004013061523\n",
      "Loss: 3.6988096237182617\n",
      "Loss: 3.7130141258239746\n",
      "Loss: 3.824159860610962\n",
      "Loss: 3.7459206581115723\n",
      "Loss: 3.6717352867126465\n",
      "Loss: 3.8607559204101562\n",
      "Loss: 3.7164053916931152\n",
      "Loss: 3.8569211959838867\n",
      "Loss: 3.8415961265563965\n",
      "Loss: 3.956721305847168\n",
      "Loss: 3.66918683052063\n",
      "Loss: 3.7400496006011963\n",
      "Loss: 3.8303956985473633\n",
      "Loss: 3.8280742168426514\n",
      "Loss: 3.8872275352478027\n",
      "Loss: 3.922959804534912\n",
      "Loss: 3.747351884841919\n",
      "Loss: 3.686636447906494\n",
      "Loss: 3.7471349239349365\n",
      "Loss: 3.649672508239746\n",
      "Loss: 3.6498239040374756\n",
      "Loss: 3.7332122325897217\n",
      "Loss: 3.7020857334136963\n",
      "Loss: 3.8627591133117676\n",
      "Loss: 3.8559422492980957\n",
      "Loss: 3.743752956390381\n",
      "Loss: 4.105913162231445\n",
      "Loss: 3.8060660362243652\n",
      "Loss: 3.759873628616333\n",
      "Loss: 3.694650650024414\n",
      "Loss: 3.8150811195373535\n",
      "Loss: 3.7417104244232178\n",
      "Loss: 3.7299258708953857\n",
      "Loss: 3.68445086479187\n",
      "Loss: 3.851858377456665\n",
      "Loss: 3.95263934135437\n",
      "Loss: 3.7461743354797363\n",
      "Loss: 3.7813756465911865\n",
      "Loss: 3.9065356254577637\n",
      "Loss: 4.008878707885742\n",
      "Loss: 3.7045984268188477\n",
      "Loss: 3.78124737739563\n",
      "Loss: 3.701702356338501\n",
      "Loss: 3.664151668548584\n",
      "Loss: 3.8101351261138916\n",
      "Loss: 3.804201126098633\n",
      "Loss: 3.8045191764831543\n",
      "Loss: 3.7754907608032227\n",
      "Loss: 3.7442901134490967\n",
      "Loss: 3.7555253505706787\n",
      "Loss: 3.7953908443450928\n",
      "Loss: 3.8266642093658447\n",
      "Loss: 3.800628900527954\n",
      "Loss: 3.6809725761413574\n",
      "Loss: 3.7558510303497314\n",
      "Loss: 3.7059874534606934\n",
      "Loss: 3.8331828117370605\n",
      "Loss: 3.929645538330078\n",
      "Loss: 3.661200761795044\n",
      "Loss: 3.7395081520080566\n",
      "Loss: 3.6759040355682373\n",
      "Loss: 3.763044834136963\n",
      "Loss: 3.6423184871673584\n",
      "Loss: 3.7308807373046875\n",
      "Loss: 3.5708048343658447\n",
      "Loss: 3.8752248287200928\n",
      "Loss: 3.810908079147339\n",
      "Loss: 3.638963460922241\n",
      "Loss: 3.8367526531219482\n",
      "Loss: 3.6342084407806396\n",
      "Loss: 3.655113935470581\n",
      "Loss: 3.9109275341033936\n",
      "Loss: 3.6844558715820312\n",
      "Loss: 3.7918810844421387\n",
      "Loss: 3.801551342010498\n",
      "Loss: 3.885108709335327\n",
      "Loss: 3.8562064170837402\n",
      "Loss: 3.890845537185669\n",
      "Loss: 3.708756446838379\n",
      "Loss: 3.8102879524230957\n",
      "Loss: 3.787644863128662\n",
      "Loss: 3.823908805847168\n",
      "Loss: 3.671260118484497\n",
      "Loss: 3.7195749282836914\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[416], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x[\u001b[39m1\u001b[39m:], eos])\n\u001b[1;32m     20\u001b[0m \u001b[39m# inspect batch \u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# print(sentence_piece_tokenizer.decode(x.tolist()))\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m p \u001b[39m=\u001b[39m m(x)\n\u001b[1;32m     24\u001b[0m l \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mcross_entropy(p, y)\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, l\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/amber-transformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[414], line 27\u001b[0m, in \u001b[0;36mAmberSimpleTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 27\u001b[0m   emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(x) \u001b[39m#embedding_size --> this will output something of d sequence length times embedding size \u001b[39;00m\n\u001b[1;32m     28\u001b[0m   pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_emb[\u001b[39m0\u001b[39m:x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], :]\n\u001b[1;32m     29\u001b[0m   emb \u001b[39m=\u001b[39m emb \u001b[39m+\u001b[39m pos \u001b[39m# add positional information to the embedding \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/amber-transformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/amber-transformer/lib/python3.8/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/amber-transformer/lib/python3.8/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "m = AmberSimpleTransformer()\n",
    "opt = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
    "print(\"num_params:\", sum(p.numel() for p in m.parameters()))\n",
    "\n",
    "\n",
    "losses = []\n",
    "for _ in range(10000):\n",
    "  for idx, batch in enumerate(dl):\n",
    "\n",
    "    m.train()\n",
    "    # print(\"batch[0]\", idx, batch[0])\n",
    "    sos = torch.tensor([2])\n",
    "    eos = torch.tensor([1])\n",
    "    if idx == 2: break\n",
    "\n",
    "    x = batch[0]\n",
    "    x = torch.cat([sos, x])\n",
    "    y = torch.cat([x[1:], eos])\n",
    "    \n",
    "    # inspect batch \n",
    "    # print(sentence_piece_tokenizer.decode(x.tolist()))\n",
    "\n",
    "    p = m(x)\n",
    "    l = torch.nn.functional.cross_entropy(p, y)\n",
    "    print(\"Loss:\", l.item())\n",
    "    if idx%1000 == 0:\n",
    "      losses.append(l.item())\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "print(type(losses))\n",
    "\n",
    "# torch.save(m, f'models/{uuid}_final.pth')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the losses to see how well the training has gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the losses\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define different sampling strategies (how do you choose the next token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define different sampling approaches to test out\n",
    "def greedy_decode(next_token_options, x):\n",
    "  next_token = torch.argmax(next_token_options) # find index of maximum value in each row \n",
    "  x = torch.cat([x, next_token.unsqueeze(0)]) #here we're just adding the last, converted to 2D\n",
    "  return x\n",
    "\n",
    "def random_sample(next_token_options, x):\n",
    "  next_token = torch.multinomial(next_token_options, 1)  # Sample the next token using a multinomial distribution\n",
    "  x = torch.cat([x, next_token])  # Add the sampled token to the sequence\n",
    "  return x\n",
    "\n",
    "def top_k_sample(next_token_options, x, k=5):\n",
    "  top_k_values, top_k_indices = torch.topk(next_token_options, k)  # Get the top-k values and indices\n",
    "  next_token = top_k_indices[torch.randint(k, (1,))]\n",
    "  x = torch.cat([x, next_token])  # Add the selected token to the sequence\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some text from a random start token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find next token after: ne\n",
      "Generate: ne                            \n",
      "find next token after: th\n",
      "Generate: th                             \n",
      "find next token after: T\n",
      "Generate: T                             \n",
      "find next token after: k\n",
      "Generate: k                            \n",
      "find next token after: u\n",
      "Generate: u                            \n",
      "find next token after: o\n",
      "Generate: o                             \n",
      "find next token after: ogether\n",
      "Generate: ogether                            \n",
      "find next token after: \"\n",
      "Generate: \"                             \n",
      "find next token after: m\n",
      "Generate: m                             \n",
      "find next token after: S\n",
      "Generate: S                            \n"
     ]
    }
   ],
   "source": [
    "m.eval()\n",
    "for _ in range(0,10):\n",
    "  random_start_token = random.randint(3, vocab_size-1)\n",
    "  x = sentence_piece_tokenizer.decode([random_start_token])\n",
    "  x = torch.cat([sos, torch.tensor(sentence_piece_tokenizer.encode(x))])\n",
    "  print(\"find next token after:\", sentence_piece_tokenizer.decode(x.tolist()))\n",
    "  while True:\n",
    "    p = m(x) #logits\n",
    "    p = torch.nn.functional.softmax(p, dim=1) #convert logits to likelihood out of 1\n",
    "    next_token_options = p[-1]\n",
    "    x = greedy_decode(next_token_options, x)\n",
    "    if x[-1] == 1 or len(p.tolist()) == 30: break #stop generating if last token is end token or sequence len is 17\n",
    "  print(\"Generate:\", sentence_piece_tokenizer.decode(x.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
